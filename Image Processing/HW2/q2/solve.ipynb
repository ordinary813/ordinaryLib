{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matches is of (3|4 X 2 X 2) size. Each row is a match - pair of (kp1,kp2) where kpi = (x,y)\n",
    "def get_transform(matches, is_affine, img_index):\n",
    "\t# Flatten matches to extract src_points and dst_points\n",
    "    specific_matches = matches[img_index - 2]\n",
    "\n",
    "    src_points = specific_matches[:, 0]  # All (x, y) for img1\n",
    "    dst_points = specific_matches[:, 1]  # All (x, y) for img2\n",
    "\n",
    "    if is_affine:\n",
    "        # Compute the affine transformation\n",
    "        T, _ = cv2.estimateAffinePartial2D(src_points, dst_points)\n",
    "    else:\n",
    "        # Compute the homography\n",
    "        T, _ = cv2.findHomography(src_points, dst_points, method=cv2.RANSAC)\n",
    "    \n",
    "    return T\n",
    "\n",
    "def stitch(img1, img2, transform):\n",
    "    height, width = img1.shape[:2]\n",
    "    output_size = (width, height)\n",
    "\n",
    "    warped_img2 =  inverse_transform_target_image(img2, transform, output_size)\n",
    "    \n",
    "    mask = cv2.cvtColor(warped_img2, cv2.COLOR_BGR2GRAY) > 0  # Non-black pixels are True\n",
    "    mask = mask.astype(np.uint8)  # Convert to binary mask (0 or 1)\n",
    "\n",
    "    stitched_img = img1.copy()\n",
    "    for c in range(3):  # Iterate over color channels\n",
    "        stitched_img[:, :, c] = stitched_img[:, :, c] * (1 - mask) + warped_img2[:, :, c] * mask\n",
    "\n",
    "    return stitched_img\n",
    "\n",
    "\n",
    "# Output size is (w,h)\n",
    "def inverse_transform_target_image(target_img, original_transform, output_size):\t\n",
    "    if original_transform.shape == (2, 3):  # Affine transformation\n",
    "        inverse_transform = cv2.invertAffineTransform(original_transform)\n",
    "        warped_img = cv2.warpAffine(target_img, inverse_transform, output_size)\n",
    "    elif original_transform.shape == (3, 3):  # Homography\n",
    "        inverse_transform = np.linalg.inv(original_transform)\n",
    "        warped_img = cv2.warpPerspective(target_img, inverse_transform, output_size)\n",
    "    return warped_img\n",
    "\n",
    "# returns list of pieces file names\n",
    "def prepare_puzzle(puzzle_dir):\n",
    "\tedited = os.path.join(puzzle_dir, 'abs_pieces')\n",
    "\tif os.path.exists(edited):\n",
    "\t\tshutil.rmtree(edited)\n",
    "\tos.mkdir(edited)\n",
    "\t\n",
    "\taffine = 4 - int(\"affine\" in puzzle_dir)\n",
    "\t\n",
    "\tmatches_data = os.path.join(puzzle_dir, 'matches.txt')\n",
    "\tn_images = len(os.listdir(os.path.join(puzzle_dir, 'pieces')))\n",
    "\n",
    "\tmatches = np.loadtxt(matches_data, dtype=np.int64).reshape(n_images-1,affine,2,2)\n",
    "\t\n",
    "\treturn matches, affine == 3, n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puzzle_dir = 'puzzles/puzzle_affine_2'\n",
    "\n",
    "# img1 = cv2.imread(os.path.join(puzzle_dir, 'pieces/piece_1.jpg'))  # Image with black background and real picture window\n",
    "# img2 = cv2.imread(os.path.join(puzzle_dir, 'pieces/piece_2.jpg'))  # Image to be added\n",
    "\n",
    "# matches, is_affine, n = prepare_puzzle(puzzle_dir)\n",
    "# T = get_transform(matches, is_affine)\n",
    "\n",
    "# src_points = matches[:, :, 0, :].reshape(-1, 2)  # All (x, y) for img1\n",
    "# dst_points = matches[:, :, 1, :].reshape(-1, 2)  # All (x, y) for img2\n",
    "\n",
    "# for point in src_points:\n",
    "#     cv2.circle(img1, tuple(map(int, point)), 5, (255, 0, 0), -1)\n",
    "# for point in dst_points:\n",
    "#     cv2.circle(img2, tuple(map(int, point)), 5, (0, 255, 0), -1)\n",
    "# cv2.imshow('Image 1 Keypoints', img1)\n",
    "# cv2.imshow('Image 2 Keypoints', img2)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches, is_affine, n = prepare_puzzle('puzzles/puzzle_affine_1')\n",
    "# transform = get_transform(matches, is_affine)\n",
    "\n",
    "# print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle_dir = 'puzzles/puzzle_affine_2'\n",
    "\n",
    "img1 = cv2.imread(os.path.join(puzzle_dir, 'pieces/piece_1.jpg'))  # Image with black background and real picture window\n",
    "\n",
    "matches, is_affine, n = prepare_puzzle(puzzle_dir)\n",
    "stitched_img = img1\n",
    "\n",
    "\n",
    "transform = get_transform(matches, is_affine, img_index=2)\n",
    "\n",
    "for i in range(2, n + 1):  # Images from piece_2.jpg to piece_n.jpg\n",
    "        img2 = cv2.imread(os.path.join(puzzle_dir, 'pieces', f'piece_{i}.jpg'))\n",
    "        \n",
    "        # Get the transformation matrix for the current image pair (piece_1 vs piece_2, piece_1 vs piece_3, etc.)\n",
    "        transform = get_transform(matches, is_affine, img_index=i)\n",
    "\n",
    "        # Stitch the current img2 onto the stitched_img\n",
    "        stitched_img = stitch(stitched_img, img2, transform)\n",
    "\n",
    "# Save or display the stitched result\n",
    "cv2.imwrite('stitched_image.jpg', stitched_img)\n",
    "cv2.imshow('Stitched Image', stitched_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
