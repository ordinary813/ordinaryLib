{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.fft import fft2, ifft2, fftshift, ifftshift\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_down(image, resize_ratio):\n",
    "    f_transform = fft2(image)\n",
    "    f_transform = fftshift(f_transform)\n",
    "\n",
    "    # 0 < ratio < 1\n",
    "    new_h, new_w = int(image.shape[0] * resize_ratio), int(image.shape[1] * resize_ratio)\n",
    "    center_h, center_w = f_transform.shape[0] // 2, f_transform.shape[1] // 2\n",
    "\n",
    "    # crop the frequency spectrum\n",
    "    cropped_transform = f_transform[center_h - new_h // 2:center_h + new_h // 2, \n",
    "                                    center_w - new_w // 2:center_w + new_w // 2]\n",
    "    \n",
    "    # after cropping, inverse fft\n",
    "    downscaled_image = np.abs(ifft2(ifftshift(cropped_transform)))\n",
    "    return downscaled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_up(image, resize_ratio):\n",
    "    f_transform = fft2(image)\n",
    "    f_transform = fftshift(f_transform)\n",
    "\n",
    "    # ratio > 1\n",
    "    new_h, new_w = int(image.shape[0] * resize_ratio), int(image.shape[1] * resize_ratio)\n",
    "    center_h, center_w = f_transform.shape[0] // 2, f_transform.shape[1] // 2\n",
    "\n",
    "    # define the wanted image size\n",
    "    expanded_transform = np.zeros((new_h, new_w), dtype=complex)\n",
    "\n",
    "    orig_h, orig_w = f_transform.shape\n",
    "    orig_center_h, orig_center_w = orig_h // 2, orig_w // 2\n",
    "\n",
    "    # copy the fourier spectrum to the scaled up image, from the center\n",
    "    expanded_transform[center_h - orig_center_h:center_h + orig_center_h,\n",
    "                       center_w - orig_center_w:center_w + orig_center_w] = f_transform\n",
    "    \n",
    "    # shift back to 0,0 and inverse fft\n",
    "    expanded_spectrum = ifftshift(expanded_spectrum)\n",
    "    scaled_up_image = np.abs(ifft2(expanded_spectrum))\n",
    "    \n",
    "    return scaled_up_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncc_2d(image, pattern):\n",
    "\tpattern_mean = np.mean(pattern)\n",
    "\tpattern_std = np.std(pattern)\n",
    "\n",
    "\twindows = np.lib.stride_tricks.sliding_window_view(image, pattern.shape)\n",
    "\n",
    "\twindow_means = np.mean(windows, axis=(-2, -1))\n",
    "\twindow_stds = np.std(windows, axis=(-2, -1))\n",
    "\n",
    "\tncc_result = np.sum((windows - window_means[..., np.newaxis, np.newaxis]) * \n",
    "\t\t\t\t\t\t(pattern - pattern_mean), axis=(-2, -1)) / (window_stds * pattern_std)\n",
    "\n",
    "\t# Handle division by zero when window standard deviation is zero\n",
    "\tncc_result = np.where(window_stds != 0, ncc_result, 0)\n",
    "\n",
    "\treturn ncc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(image, pattern):\n",
    "\tplt.subplot(2, 3, 1)\n",
    "\tplt.title('Image')\n",
    "\tplt.imshow(image, cmap='gray')\n",
    "\t\t\n",
    "\tplt.subplot(2, 3, 3)\n",
    "\tplt.title('Pattern')\n",
    "\tplt.imshow(pattern, cmap='gray', aspect='equal')\n",
    "\t\n",
    "\tncc = ncc_2d(image, pattern)\n",
    "\t\n",
    "\tplt.subplot(2, 3, 5)\n",
    "\tplt.title('Normalized Cross-Correlation Heatmap')\n",
    "\tplt.imshow(ncc ** 2, cmap='coolwarm', vmin=0, vmax=1, aspect='auto') \n",
    "\t\n",
    "\tcbar = plt.colorbar()\n",
    "\tcbar.set_label('NCC Values')\n",
    "\t\t\n",
    "\tplt.show()\n",
    "\n",
    "def draw_matches(image, matches, pattern_size):\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\tfor point in matches:\n",
    "\t\ty, x = point\n",
    "\t\ttop_left = (int(x - pattern_size[1]/2), int(y - pattern_size[0]/2))\n",
    "\t\tbottom_right = (int(x + pattern_size[1]/2), int(y + pattern_size[0]/2))\n",
    "\t\tcv2.rectangle(image, top_left, bottom_right, (255, 0, 0), 1)\n",
    "\t\n",
    "\tplt.imshow(image, cmap='gray')\n",
    "\tplt.show()\n",
    "\t\n",
    "\tcv2.imwrite(f\"{CURR_IMAGE}_result.jpg\", image)\n",
    "\n",
    "\n",
    "CURR_IMAGE = \"students\"\n",
    "\n",
    "image = cv2.imread(f'{CURR_IMAGE}.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "pattern = cv2.imread('template.jpg')\n",
    "pattern = cv2.cvtColor(pattern, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "############# DEMO #############\n",
    "display(image, pattern)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############# Students #############\n",
    "\n",
    "image_scaled = image\n",
    "pattern_scaled =  pattern\n",
    "\n",
    "display(image_scaled, pattern_scaled)\n",
    "\n",
    "ncc = ncc_2d(image, pattern)\n",
    "threshold = 0.7  # **Added a threshold for match selection**\n",
    "real_matches = np.argwhere(ncc >= threshold)\n",
    "\n",
    "######### DONT CHANGE THE NEXT TWO LINES #########\n",
    "real_matches[:,0] += pattern_scaled.shape[0] // 2\t\t\t# if pattern was not scaled, replace this with \"pattern\"\n",
    "real_matches[:,1] += pattern_scaled.shape[1] // 2\t\t\t# if pattern was not scaled, replace this with \"pattern\"\n",
    "\n",
    "# If you chose to scale the original image, make sure to scale back the matches in the inverse resize ratio.\n",
    "\n",
    "draw_matches(image, real_matches, pattern_scaled.shape)\t# if pattern was not scaled, replace this with \"pattern\"\n",
    "\n",
    "\n",
    "\n",
    "CURR_IMAGE = \"thecrew\"\n",
    "image = cv2.imread(f'{CURR_IMAGE}.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "############# Crew #############\n",
    "\n",
    "image_scaled = image\n",
    "pattern_scaled = pattern\n",
    "\n",
    "display(image_scaled, pattern_scaled)\n",
    "\n",
    "ncc = ncc_2d(image, pattern)\n",
    "threshold = 0.7  # **Added a threshold for match selection**\n",
    "real_matches = np.argwhere(ncc >= threshold)\n",
    "\n",
    "######### DONT CHANGE THE NEXT TWO LINES #########\n",
    "real_matches[:,0] += pattern_scaled.shape[0] // 2\t\t\t# if pattern was not scaled, replace this with \"pattern\"\n",
    "real_matches[:,1] += pattern_scaled.shape[1] // 2\t\t\t# if pattern was not scaled, replace this with \"pattern\"\n",
    "\n",
    "# If you chose to scale the original image, make sure to scale back the matches in the inverse resize ratio.\n",
    "\n",
    "draw_matches(image, real_matches, pattern_scaled.shape)\t# if pattern was not scaled, replace this with \"pattern\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
