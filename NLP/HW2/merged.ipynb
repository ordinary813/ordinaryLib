{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trigram_LM:\n",
    "\n",
    "    def __init__(self, protocol_type, file_path):\n",
    "        self.protocol_type = protocol_type\n",
    "        self.corpus = None  # DataFrame to store the relvant sentences from corpus\n",
    "        self.unigrams = {}\n",
    "        self.bigrams = {}\n",
    "        self.trigrams = {}\n",
    "        self.load_corpus(file_path) # load the corpus\n",
    "        self.calculate_counts() #calc bigrams, trigrams and unigrams\n",
    "\n",
    "    def load_corpus(self, file_path):\n",
    "        try:\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "            # Filter the DataFrame based on the protocol type\n",
    "            self.corpus = df[df['protocol_type'] == self.protocol_type]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading corpus: {e}\")\n",
    "\n",
    "    # Counts appearances of every collocation of size 1,2,3 in the corpus\n",
    "    def calculate_counts(self):\n",
    "        # Iterate over each row in the corpus\n",
    "        for index, row in self.corpus.iterrows():\n",
    "            sentence = row['sentence_text']\n",
    "            tokens = sentence.split()  # Tokenize the sentence\n",
    "\n",
    "            # Calculate unigram count frequencies\n",
    "            for token in tokens:\n",
    "                if token not in self.unigrams:\n",
    "                    self.unigrams[token] = 0\n",
    "                self.unigrams[token] += 1\n",
    "\n",
    "            # Calculate bigram count frequencies\n",
    "            for i in range(len(tokens) - 1):\n",
    "                bigram = (tokens[i], tokens[i + 1])\n",
    "                if bigram not in self.bigrams:\n",
    "                    self.bigrams[bigram] = 0\n",
    "                self.bigrams[bigram] += 1\n",
    "\n",
    "            # Calculate trigram count frequencies\n",
    "            for i in range(len(tokens) - 2):\n",
    "                trigram = (tokens[i], tokens[i + 1], tokens[i + 2])\n",
    "                if trigram not in self.trigrams:\n",
    "                    self.trigrams[trigram] = 0\n",
    "                self.trigrams[trigram] += 1\n",
    "\n",
    "    # Input: sentence\n",
    "    # Output: log probability of the sentence to appear in the corpus\n",
    "    def calculate_prob_of_sentence(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        tokens = [\"<s_0>\", \"<s_1>\"] + tokens  # Add start tokens\n",
    "        log_prob = 0\n",
    "\n",
    "        # weights for the interpolation (give more value to trigram that has more context)\n",
    "        lambda_1 = 0.7\n",
    "        lambda_2 = 0.2\n",
    "        lambda_3 = 0.1\n",
    "\n",
    "        V = len(self.unigrams)  # number of unique words in the corpus\n",
    "\n",
    "        # Start from the third token\n",
    "        for i in range(2, len(tokens)):\n",
    "            unigram = tokens[i]  # the current token\n",
    "            bigram = (tokens[i - 1], tokens[i])  # current and previous tokens\n",
    "            trigram = (tokens[i - 2], tokens[i - 1], tokens[i])  # current and two previous tokens\n",
    "\n",
    "            # Add 1 for Laplace smoothing\n",
    "            trigram_count = self.trigrams.get(trigram, 0) + 1\n",
    "            bigram_count = self.bigrams.get(bigram, 0) + 1\n",
    "            unigram_count = self.unigrams.get(unigram, 0) + 1\n",
    "\n",
    "            # Add V for smoothing\n",
    "            trigram_probability = trigram_count / (self.bigrams.get((tokens[i - 2], tokens[i - 1]), 0) + V)\n",
    "            bigram_probability = bigram_count / (self.unigrams.get(tokens[i - 1], 0) + V)\n",
    "            unigram_probability = unigram_count / (sum(self.unigrams.values()) + V)\n",
    "\n",
    "            # Apply linear interpolation\n",
    "            prob = lambda_1 * trigram_probability + lambda_2 * bigram_probability + lambda_3 * unigram_probability\n",
    "\n",
    "            # Log probability (avoid log(0))\n",
    "            log_prob += (0 if prob == 0 else math.log(prob))\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def generate_next_token(self, sentence):\n",
    "\n",
    "        # Split the input sentence into tokens\n",
    "        tokens = sentence.split()\n",
    "        tokens = [\"<s_0>\", \"<s_1>\"] + tokens  # Add start tokens\n",
    "\n",
    "        highest_prob = -float('inf')\n",
    "        best_token = None\n",
    "\n",
    "        # weights for the interpolation\n",
    "        lambda_1 = 0.7\n",
    "        lambda_2 = 0.2\n",
    "        lambda_3 = 0.1\n",
    "\n",
    "        V = len(self.unigrams)  # number of unique words in the corpus\n",
    "\n",
    "        # Loop through all potential next tokens\n",
    "        for token in self.unigrams:\n",
    "            unigram = token\n",
    "            bigram = (tokens[-1], token)\n",
    "            trigram = (tokens[-2], tokens[-1], token)\n",
    "\n",
    "            # Add 1 for Laplace smoothing\n",
    "            trigram_count = self.trigrams.get(trigram, 0) + 1\n",
    "            bigram_count = self.bigrams.get(bigram, 0) + 1\n",
    "            unigram_count = self.unigrams.get(unigram, 0) + 1\n",
    "\n",
    "            # Add V for smoothing\n",
    "            trigram_probability = trigram_count / (self.bigrams.get((tokens[-2], tokens[-1]), 0) + V)\n",
    "            bigram_probability = bigram_count / (self.unigrams.get(tokens[-1], 0) + V)\n",
    "            unigram_probability = unigram_count / (sum(self.unigrams.values()) + V)\n",
    "\n",
    "            # Linear interpolation of trigram, bigram, and unigram probabilities\n",
    "            prob = lambda_1 * trigram_probability + lambda_2 * bigram_probability + lambda_3 * unigram_probability\n",
    "\n",
    "            # Update the best token\n",
    "            if prob > highest_prob:\n",
    "                highest_prob = prob\n",
    "                best_token = token\n",
    "\n",
    "        return best_token, highest_prob\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return [' '.join(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "def compute_idf(protocol_collocations, total_docs):\n",
    "    doc_counts = {}\n",
    "\n",
    "    # for every collocation in all documents we increment when we saw the collocation\n",
    "    # by the end of this loop each collocation should have its frequency in all documents\n",
    "    for doc_collocations in protocol_collocations:\n",
    "        for collocation in doc_collocations:\n",
    "            doc_counts[collocation] = doc_counts.get(collocation, 0) + 1\n",
    "\n",
    "    idf_scores = {}\n",
    "\n",
    "    # compute IDF for each collocation\n",
    "    for collocation, doc_count in doc_counts.items():\n",
    "        idf_scores[collocation] = log(total_docs / (doc_count))\n",
    "\n",
    "    return idf_scores\n",
    "\n",
    "\n",
    "def compute_tf(doc_collocations):\n",
    "    total_count = len(doc_collocations)\n",
    "    collocation_counts = {}\n",
    "    \n",
    "    # Count occurrences of each collocation in doc_collocations\n",
    "    for coll in doc_collocations:\n",
    "        collocation_counts[coll] = collocation_counts.get(coll, 0) + 1\n",
    "    \n",
    "    # Compute TF scores\n",
    "    tf_scores = {coll: count / total_count for coll, count in collocation_counts.items()}\n",
    "\n",
    "    return tf_scores\n",
    "\n",
    "def compute_tfidf(tf_scores, idf_scores):\n",
    "    tfidf_scores = {}\n",
    "    for coll, tf in tf_scores.items():\n",
    "        tfidf_scores[coll] = tf * idf_scores.get(coll, 0)\n",
    "    return tfidf_scores\n",
    "\n",
    "# Input: \n",
    "#   corpus_df: a dataframe containing the corpus' data\n",
    "#   k: number of top collocations\n",
    "#   n: length of collocations\n",
    "#   t: min threshold for the amount of collocations\n",
    "# Output:\n",
    "#   collocation:grade list from the corpus\n",
    "def get_k_n_t_collocations(corpus_df, k, n, t, type):\n",
    "\n",
    "    #produce all collocations of length n\n",
    "    corpus_df['collocations'] = corpus_df['sentence_text'].apply(lambda x: generate_ngrams(x, n))\n",
    "\n",
    "    # place all collocations in a dictionary of structure <Collocation>: <Count>\n",
    "    collocation_counts = {}\n",
    "    for coll_list in corpus_df['collocations']:\n",
    "        for coll in coll_list:\n",
    "            collocation_counts[coll] = collocation_counts.get(coll, 0) + 1\n",
    "\n",
    "    if type == \"frequency\":\n",
    "        # only include collcations that appear more than <t>\n",
    "        filtered_collocations = {coll: count for coll, count in collocation_counts.items() if count >= t}\n",
    "    elif type == \"tfidf\":\n",
    "\n",
    "        total_docs = len(corpus_df['protocol_name'].unique())\n",
    "\n",
    "        # group by protocol docs\n",
    "        grouped = corpus_df.groupby('protocol_name')['collocations']\n",
    "        protocol_collocations = grouped.apply(lambda x: sum(x, []))\n",
    "\n",
    "        idf_scores = compute_idf(protocol_collocations, total_docs)\n",
    "\n",
    "        tfidf_scores = {}\n",
    "        collocation_counts = {}\n",
    "\n",
    "        print(\"Processing protocols...\")\n",
    "\n",
    "        for protocol_name, collocations in grouped:\n",
    "            print(f\"\\rProcessing {protocol_name:<30}\", end='', flush=True)\n",
    "            # list of collocations for the current document/protocol\n",
    "            doc_collocations = sum(collocations, [])\n",
    "\n",
    "            for coll in doc_collocations:\n",
    "                collocation_counts[coll] = collocation_counts.get(coll, 0) + 1\n",
    "\n",
    "            tf_scores = compute_tf(doc_collocations)\n",
    "\n",
    "            # Compute TF-IDF for collocations in this document\n",
    "            tfidf = compute_tfidf(tf_scores, idf_scores)\n",
    "\n",
    "            for coll, score in tfidf.items():\n",
    "                tfidf_scores[coll] = tfidf_scores.get(coll, 0) + score\n",
    "\n",
    "        # Only include collocations that have a score >= t\n",
    "        filtered_collocations = {coll: score for coll, score in tfidf_scores.items() if collocation_counts.get(coll, 0) >= t}\n",
    "\n",
    "    sorted_collocations = sorted(filtered_collocations.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return sorted_collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_word(n):\n",
    "    match n:\n",
    "        case 2:\n",
    "            return \"Two\"\n",
    "        case 3:\n",
    "            return \"Three\"\n",
    "        case 4:\n",
    "            return \"Four\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: list of strings and a percentage x\n",
    "# Output: list of strings after masking x% of the tokens\n",
    "def mask_tokens_in_sentences(sentences, x):\n",
    "    masked_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        num_tokens_to_mask = int(len(tokens) * (x / 100))\n",
    "        if num_tokens_to_mask == 0:\n",
    "            num_tokens_to_mask = 1\n",
    "        tokens_to_mask = random.sample(range(len(tokens)), num_tokens_to_mask)\n",
    "\n",
    "        masked_tokens = [\"[*]\" if i in tokens_to_mask else token for i, token in enumerate(tokens)]\n",
    "        masked_sentences.append(\" \".join(masked_tokens))\n",
    "    return masked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a dataframe, amount of entries to mask with [*], and a percentage x\n",
    "# Output: the dataframe after applying the mask\n",
    "def mask_corpus(corpus_df, amount_to_mask, x, original_path ='', masked_path=''):\n",
    "    if amount_to_mask > len(corpus_df):\n",
    "        amount_to_mask = len(corpus_df)\n",
    "\n",
    "    # copy dataframe to ensure integrity of the original dataframe\n",
    "    df_copy = corpus_df.copy()\n",
    "    # mask_indices = random.sample(range(len(df_copy)), amount_to_mask)\n",
    "\n",
    "    more_than_5 = [i for i in range(len(df_copy)) if len(re.findall(r'\\S+', df_copy.iloc[i]['sentence_text'])) >= 5]\n",
    "\n",
    "    # pick senetences with more than 5 tokens\n",
    "    mask_indices = random.sample(range(len(more_than_5)), amount_to_mask)\n",
    "\n",
    "    sentences_to_mask = [df_copy.iloc[i]['sentence_text'] for i in mask_indices]\n",
    "\n",
    "    masked_sentences = mask_tokens_in_sentences(sentences_to_mask, x)\n",
    "\n",
    "    for idx, i in enumerate(mask_indices):\n",
    "        df_copy.loc[df_copy.index[i], 'sentence_text'] = masked_sentences[idx]\n",
    "    \n",
    "    # print only when necessary\n",
    "    if original_path != '' and masked_path != '':\n",
    "        with open(original_path, 'w', encoding='utf-8') as original_file, \\\n",
    "            open(masked_path, 'w', encoding='utf-8') as masked_file:\n",
    "            \n",
    "            for index in mask_indices:\n",
    "                original_file.write(f\"{corpus_df['sentence_text'].iloc[index]}\\n\")\n",
    "                masked_file.write(f\"{df_copy['sentence_text'].iloc[index]}\\n\")\n",
    "\n",
    "    return df_copy, mask_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'knesset_corpus.jsonl'\n",
    "output_file = 'top_collocations.txt'\n",
    "\n",
    "exec = 3\n",
    "\n",
    "#############################################\n",
    "############### Exercise 2 ##################\n",
    "#############################################\n",
    "\n",
    "k = 10          # top 10 collocations\n",
    "ns = [2,3,4]    # n-grams\n",
    "t = 5           # minimum of <t> counts for an n-gram\n",
    "types = ['frequency','tfidf']\n",
    "\n",
    "trigram_model_committee = Trigram_LM(\"committee\", corpus_path)\n",
    "trigram_model_plenary = Trigram_LM(\"plenary\", corpus_path)\n",
    "\n",
    "if exec == 2:\n",
    "    #save the output into a file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for n in ns:\n",
    "            f.write(f\"{to_word(n)}-gram collocations:\\n\")\n",
    "            for score_type in types:\n",
    "                if score_type == 'frequency':\n",
    "                    f.write(f\"Frequency:\\n\")\n",
    "                elif score_type == 'tfidf':\n",
    "                    f.write(f\"TF-IDF:\\n\")\n",
    "                \n",
    "                for model_name, model_corpus in [\n",
    "                    (\"Committee corpus\", trigram_model_committee.corpus),\n",
    "                    (\"Plenary corpus\", trigram_model_plenary.corpus)\n",
    "                ]:\n",
    "\n",
    "                    f.write(f\"{model_name}:\\n\")\n",
    "                    # Get top collocations for the current configuration\n",
    "                    result = get_k_n_t_collocations(model_corpus,k, n, t, score_type)\n",
    "                    for collocation in result:\n",
    "                        f.write(f\"{collocation[0]}\\n\")\n",
    "                    f.write(\"\\n\")  # Empty line between sections\n",
    "            f.write(\"\\n\")  # Empty line between n-gram categories\n",
    "\n",
    "    print(f\"\\nComplete, Output file: {output_file}\", flush=True)\n",
    "\n",
    "#############################################\n",
    "############### Exercise 3 ##################\n",
    "#############################################\n",
    "\n",
    "original_ouput_path = 'original_sampled_sents.txt'\n",
    "masked_ouput_path = 'masked_sampled_sents.txt'\n",
    "\n",
    "# print to file\n",
    "masked_corpus_comittee, mask_indices_committee = mask_corpus(trigram_model_committee.corpus, 10, 10, original_ouput_path, masked_ouput_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
