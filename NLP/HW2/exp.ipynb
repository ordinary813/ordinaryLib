{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from math import log\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return [' '.join(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "def compute_idf(protocol_collocations, total_docs):\n",
    "    doc_counts = {}\n",
    "\n",
    "    # for every collocation in all documents we increment when we saw the collocation\n",
    "    # by the end of this loop each collocation should have its frequency in all documents\n",
    "    for doc_collocations in protocol_collocations:\n",
    "        for collocation in doc_collocations:\n",
    "            doc_counts[collocation] = doc_counts.get(collocation, 0) + 1\n",
    "\n",
    "    idf_scores = {}\n",
    "\n",
    "    # compute IDF for each collocation\n",
    "    for collocation, doc_count in doc_counts.items():\n",
    "        idf_scores[collocation] = log(total_docs / (doc_count))\n",
    "\n",
    "    return idf_scores\n",
    "\n",
    "\n",
    "def compute_tf(doc_collocations):\n",
    "    total_count = len(doc_collocations)\n",
    "    collocation_counts = {}\n",
    "    \n",
    "    # Count occurrences of each collocation in doc_collocations\n",
    "    for coll in doc_collocations:\n",
    "        collocation_counts[coll] = collocation_counts.get(coll, 0) + 1\n",
    "    \n",
    "    # Compute TF scores\n",
    "    tf_scores = {coll: count / total_count for coll, count in collocation_counts.items()}\n",
    "\n",
    "    return tf_scores\n",
    "\n",
    "def compute_tfidf(tf_scores, idf_scores):\n",
    "    tfidf_scores = {}\n",
    "    for coll, tf in tf_scores.items():\n",
    "        tfidf_scores[coll] = tf * idf_scores.get(coll, 0)\n",
    "    return tfidf_scores\n",
    "\n",
    "# Input: \n",
    "#   corpus_df: a dataframe containing the corpus' data\n",
    "#   k: number of top collocations\n",
    "#   n: length of collocations\n",
    "#   t: min threshold for the amount of collocations\n",
    "# Output:\n",
    "#   collocation:grade list from the corpus\n",
    "def get_k_n_t_collocations(corpus_df, k, n, t, type):\n",
    "\n",
    "    #produce all collocations of length n\n",
    "    corpus_df['collocations'] = corpus_df['sentence_text'].apply(lambda x: generate_ngrams(x, n))\n",
    "\n",
    "    # place all collocations in a dictionary of structure <Collocation>: <Count>\n",
    "    collocation_counts = {}\n",
    "    for coll_list in corpus_df['collocations']:\n",
    "        for coll in coll_list:\n",
    "            collocation_counts[coll] = collocation_counts.get(coll, 0) + 1\n",
    "\n",
    "    if type == \"frequency\":\n",
    "        # only include collcations that appear more than <t>\n",
    "        filtered_collocations = {coll: count for coll, count in collocation_counts.items() if count >= t}\n",
    "    elif type == \"tfidf\":\n",
    "\n",
    "        total_docs = len(corpus_df['protocol_name'].unique())\n",
    "\n",
    "        # group by protocol docs\n",
    "        grouped = corpus_df.groupby('protocol_name')['collocations']\n",
    "        protocol_collocations = grouped.apply(lambda x: sum(x, []))\n",
    "\n",
    "        idf_scores = compute_idf(protocol_collocations, total_docs)\n",
    "\n",
    "        tfidf_scores = {}\n",
    "        collocation_counts = {}\n",
    "\n",
    "        for protocol_name, collocations in grouped:\n",
    "            print(\"--------------\")\n",
    "            print(protocol_name)\n",
    "            # list of collocations for the current document/protocol\n",
    "            doc_collocations = sum(collocations, [])\n",
    "\n",
    "            for coll in doc_collocations:\n",
    "                collocation_counts[coll] = collocation_counts.get(coll, 0) + 1\n",
    "\n",
    "            tf_scores = compute_tf(doc_collocations)\n",
    "            print(\"tf_scores computed\")\n",
    "\n",
    "            # Compute TF-IDF for collocations in this document\n",
    "            tfidf = compute_tfidf(tf_scores, idf_scores)\n",
    "            print(\"tfidf_scores computed\")\n",
    "\n",
    "            for coll, score in tfidf.items():\n",
    "                tfidf_scores[coll] = tfidf_scores.get(coll, 0) + score\n",
    "            print(\"tf_scores added\")\n",
    "\n",
    "        # Only include collocations that have a score >= t\n",
    "        filtered_collocations = {coll: score for coll, score in tfidf_scores.items() if collocation_counts.get(coll, 0) >= t}\n",
    "\n",
    "    sorted_collocations = sorted(filtered_collocations.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return sorted_collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'knesset_corpus.jsonl'\n",
    "output_file = 'top_collocations.txt'\n",
    "k = 10  # top 10 collocations\n",
    "n = 2   # bigrams\n",
    "t = 5   # minimum of <t> counts for an n-gram\n",
    "type = 'tfidf'\n",
    "\n",
    "#load corpus to df\n",
    "with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "corpus_df = pd.DataFrame(data)\n",
    "\n",
    "result = get_k_n_t_collocations(corpus_df, k, n, t, type)\n",
    "\n",
    "#save the output into a file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for collocation, score in result:\n",
    "        f.write(f\"{collocation}: {score}\\n\")\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Input: list of strings and a percentage x\n",
    "# Output: list of strings after masking x% of the tokens\n",
    "def mask_tokens_in_sentences(sentences, x):\n",
    "    masked_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        num_tokens_to_mask = int(len(tokens) * (x / 100))\n",
    "        tokens_to_mask = random.sample(range(len(tokens)), num_tokens_to_mask)\n",
    "\n",
    "        masked_tokens = [\"[*]\" if i in tokens_to_mask else token for i, token in enumerate(tokens)]\n",
    "        masked_sentences.append(\" \".join(masked_tokens))\n",
    "    return masked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a dataframe, amount of entries to mask with [*], and a percentage x\n",
    "# Output: the dataframe after applying the mask\n",
    "def mask_corpus(corpus_df, amount_to_mask, x):\n",
    "    if amount_to_mask > len(corpus_df):\n",
    "        amount_to_mask = len(corpus_df)\n",
    "\n",
    "    mask_indices = random.sample(range(len(corpus_df)), amount_to_mask)\n",
    "\n",
    "    corpus_df.loc[mask_indices, 'sentence_text'] = corpus_df.loc[mask_indices, 'sentence_text'].apply(\n",
    "        lambda sentence: mask_tokens_in_sentences(sentence, x)\n",
    "    )\n",
    "    return corpus_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
