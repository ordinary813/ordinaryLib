{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trigram_LM:\n",
    "\n",
    "    def __init__(self, protocol_type, file_path):\n",
    "        self.protocol_type = protocol_type\n",
    "        self.corpus = None  # DataFrame to store the relvant sentences from corpus\n",
    "        self.unigrams = {}\n",
    "        self.bigrams = {}\n",
    "        self.trigrams = {}\n",
    "        self.load_corpus(file_path) # load the corpus\n",
    "        self.calculate_counts() #calc bigrams, trigrams and unigrams\n",
    "\n",
    "    def load_corpus(self, file_path):\n",
    "        try:\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "            # Filter the DataFrame based on the protocol type\n",
    "            self.corpus = df[df['protocol_type'] == self.protocol_type]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading corpus: {e}\")\n",
    "\n",
    "    def calculate_counts(self):\n",
    "        # Iterate over each row in the corpus\n",
    "        for index, row in self.corpus.iterrows():\n",
    "            sentence = row['sentence_text']\n",
    "            tokens = sentence.split()  # Tokenize the sentence\n",
    "\n",
    "            # Calculate unigram count frequencies\n",
    "            for token in tokens:\n",
    "                if token not in self.unigrams:\n",
    "                    self.unigrams[token] = 0\n",
    "                self.unigrams[token] += 1\n",
    "\n",
    "            # Calculate bigram count frequencies\n",
    "            for i in range(len(tokens) - 1):\n",
    "                bigram = (tokens[i], tokens[i + 1])\n",
    "                if bigram not in self.bigrams:\n",
    "                    self.bigrams[bigram] = 0\n",
    "                self.bigrams[bigram] += 1\n",
    "\n",
    "            # Calculate trigram count frequencies\n",
    "            for i in range(len(tokens) - 2):\n",
    "                trigram = (tokens[i], tokens[i + 1], tokens[i + 2])\n",
    "                if trigram not in self.trigrams:\n",
    "                    self.trigrams[trigram] = 0\n",
    "                self.trigrams[trigram] += 1\n",
    "\n",
    "    def calculate_prob_of_sentence(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        tokens = [\"<s_0>\", \"<s_1>\"] + tokens  # Add start tokens\n",
    "        log_prob = 0\n",
    "\n",
    "        # weights for the interpolation (give more value to trigram that has more context)\n",
    "        lambda_1 = 0.7\n",
    "        lambda_2 = 0.2\n",
    "        lambda_3 = 0.1\n",
    "\n",
    "        V = len(self.unigrams)  # number of unique words in the corpus\n",
    "\n",
    "        # Start from the third token\n",
    "        for i in range(2, len(tokens)):\n",
    "            unigram = tokens[i]  # the current token\n",
    "            bigram = (tokens[i - 1], tokens[i])  # current and previous tokens\n",
    "            trigram = (tokens[i - 2], tokens[i - 1], tokens[i])  # current and two previous tokens\n",
    "\n",
    "            # Add 1 for Laplace smoothing\n",
    "            trigram_count = self.trigrams.get(trigram, 0) + 1\n",
    "            bigram_count = self.bigrams.get(bigram, 0) + 1\n",
    "            unigram_count = self.unigrams.get(unigram, 0) + 1\n",
    "\n",
    "            # Add V for smoothing\n",
    "            trigram_probability = trigram_count / (self.bigrams.get((tokens[i - 2], tokens[i - 1]), 0) + V)\n",
    "            bigram_probability = bigram_count / (self.unigrams.get(tokens[i - 1], 0) + V)\n",
    "            unigram_probability = unigram_count / (sum(self.unigrams.values()) + V)\n",
    "\n",
    "            # Apply linear interpolation\n",
    "            prob = lambda_1 * trigram_probability + lambda_2 * bigram_probability + lambda_3 * unigram_probability\n",
    "\n",
    "            # Log probability (avoid log(0))\n",
    "            log_prob += (0 if prob == 0 else math.log(prob))\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def generate_next_token(self, sentence):\n",
    "\n",
    "        # Split the input sentence into tokens\n",
    "        tokens = sentence.split()\n",
    "        tokens = [\"<s_0>\", \"<s_1>\"] + tokens  # Add start tokens\n",
    "\n",
    "        highest_prob = -float('inf')\n",
    "        best_token = None\n",
    "\n",
    "        # weights for the interpolation\n",
    "        lambda_1 = 0.7\n",
    "        lambda_2 = 0.2\n",
    "        lambda_3 = 0.1\n",
    "\n",
    "        V = len(self.unigrams)  # number of unique words in the corpus\n",
    "\n",
    "        # Loop through all potential next tokens\n",
    "        for token in self.unigrams:\n",
    "            unigram = token\n",
    "            bigram = (tokens[-1], token)\n",
    "            trigram = (tokens[-2], tokens[-1], token)\n",
    "\n",
    "            # Add 1 for Laplace smoothing\n",
    "            trigram_count = self.trigrams.get(trigram, 0) + 1\n",
    "            bigram_count = self.bigrams.get(bigram, 0) + 1\n",
    "            unigram_count = self.unigrams.get(unigram, 0) + 1\n",
    "\n",
    "            # Add V for smoothing\n",
    "            trigram_probability = trigram_count / (self.bigrams.get((tokens[-2], tokens[-1]), 0) + V)\n",
    "            bigram_probability = bigram_count / (self.unigrams.get(tokens[-1], 0) + V)\n",
    "            unigram_probability = unigram_count / (sum(self.unigrams.values()) + V)\n",
    "\n",
    "            # Linear interpolation of trigram, bigram, and unigram probabilities\n",
    "            prob = lambda_1 * trigram_probability + lambda_2 * bigram_probability + lambda_3 * unigram_probability\n",
    "\n",
    "            # Update the best token\n",
    "            if prob > highest_prob:\n",
    "                highest_prob = prob\n",
    "                best_token = token\n",
    "\n",
    "        return best_token, highest_prob\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the corpus from the JSONL file\n",
    "    file_path = r\"C:\\CS\\nlp\\HW2\\knesset_corpus.jsonl\"\n",
    "    # Create an instance of Trigram_LM\n",
    "    trigram_model_committee = Trigram_LM(\"committee\", file_path)\n",
    "    # Create an instance of Trigram_LM\n",
    "    trigram_model_plenary = Trigram_LM(\"plenary\", file_path)\n",
    "\n",
    "    # # Check the corpus content\n",
    "    # if trigram_model_plenary.corpus is not None:\n",
    "    #     print(trigram_model_plenary.calculate_prob_of_sentence(trigram_model_plenary.corpus.iloc[0]['sentence_text']))\n",
    "    #     print(trigram_model_plenary.generate_next_token(trigram_model_plenary.corpus.iloc[0]['sentence_text']))\n",
    "    # else:\n",
    "    #     print(\"Corpus is empty or not loaded.\")\n",
    "    #\n",
    "    # if trigram_model_committee.corpus is not None:\n",
    "    #     print(\n",
    "    #         trigram_model_committee.calculate_prob_of_sentence(trigram_model_committee.corpus.iloc[0]['sentence_text']))\n",
    "    #     print(trigram_model_committee.generate_next_token(trigram_model_committee.corpus.iloc[0]['sentence_text']))\n",
    "    # else:\n",
    "    #     print(\"Corpus is empty or not loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return [' '.join(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "def compute_idf(protocol_collocations, total_docs):\n",
    "    doc_counts = {}\n",
    "\n",
    "    # for every collocation in all documents we increment when we saw the collocation\n",
    "    # by the end of this loop each collocation should have its frequency in all documents\n",
    "    for doc_collocations in protocol_collocations:\n",
    "        for collocation in doc_collocations:\n",
    "            doc_counts[collocation] = doc_counts.get(collocation, 0) + 1\n",
    "\n",
    "    idf_scores = {}\n",
    "\n",
    "    # compute IDF for each collocation\n",
    "    for collocation, doc_count in doc_counts.items():\n",
    "        idf_scores[collocation] = log(total_docs / (doc_count))\n",
    "\n",
    "    return idf_scores\n",
    "\n",
    "\n",
    "def compute_tf(doc_collocations):\n",
    "    total_count = len(doc_collocations)\n",
    "    collocation_counts = {}\n",
    "    \n",
    "    # Count occurrences of each collocation in doc_collocations\n",
    "    for coll in doc_collocations:\n",
    "        collocation_counts[coll] = collocation_counts.get(coll, 0) + 1\n",
    "    \n",
    "    # Compute TF scores\n",
    "    tf_scores = {coll: count / total_count for coll, count in collocation_counts.items()}\n",
    "\n",
    "    return tf_scores\n",
    "\n",
    "def compute_tfidf(tf_scores, idf_scores):\n",
    "    tfidf_scores = {}\n",
    "    for coll, tf in tf_scores.items():\n",
    "        tfidf_scores[coll] = tf * idf_scores.get(coll, 0)\n",
    "    return tfidf_scores\n",
    "\n",
    "# Input: \n",
    "#   corpus_df: a dataframe containing the corpus' data\n",
    "#   k: number of top collocations\n",
    "#   n: length of collocations\n",
    "#   t: min threshold for the amount of collocations\n",
    "# Output:\n",
    "#   collocation:grade list from the corpus\n",
    "def get_k_n_t_collocations(corpus_df, k, n, t, type):\n",
    "\n",
    "    #produce all collocations of length n\n",
    "    corpus_df['collocations'] = corpus_df['sentence_text'].apply(lambda x: generate_ngrams(x, n))\n",
    "\n",
    "    # place all collocations in a dictionary of structure <Collocation>: <Count>\n",
    "    collocation_counts = {}\n",
    "    for coll_list in corpus_df['collocations']:\n",
    "        for coll in coll_list:\n",
    "            collocation_counts[coll] = collocation_counts.get(coll, 0) + 1\n",
    "\n",
    "    if type == \"frequency\":\n",
    "        # only include collcations that appear more than <t>\n",
    "        filtered_collocations = {coll: count for coll, count in collocation_counts.items() if count >= t}\n",
    "    elif type == \"tfidf\":\n",
    "\n",
    "        total_docs = len(corpus_df['protocol_name'].unique())\n",
    "\n",
    "        # group by protocol docs\n",
    "        grouped = corpus_df.groupby('protocol_name')['collocations']\n",
    "        protocol_collocations = grouped.apply(lambda x: sum(x, []))\n",
    "\n",
    "        idf_scores = compute_idf(protocol_collocations, total_docs)\n",
    "\n",
    "        tfidf_scores = {}\n",
    "        collocation_counts = {}\n",
    "\n",
    "        for protocol_name, collocations in grouped:\n",
    "            print(\"--------------\")\n",
    "            print(protocol_name)\n",
    "            # list of collocations for the current document/protocol\n",
    "            doc_collocations = sum(collocations, [])\n",
    "\n",
    "            for coll in doc_collocations:\n",
    "                collocation_counts[coll] = collocation_counts.get(coll, 0) + 1\n",
    "\n",
    "            tf_scores = compute_tf(doc_collocations)\n",
    "            print(\"tf_scores computed\")\n",
    "\n",
    "            # Compute TF-IDF for collocations in this document\n",
    "            tfidf = compute_tfidf(tf_scores, idf_scores)\n",
    "            print(\"tfidf_scores computed\")\n",
    "\n",
    "            for coll, score in tfidf.items():\n",
    "                tfidf_scores[coll] = tfidf_scores.get(coll, 0) + score\n",
    "            print(\"tf_scores added\")\n",
    "\n",
    "        # Only include collocations that have a score >= t\n",
    "        filtered_collocations = {coll: score for coll, score in tfidf_scores.items() if collocation_counts.get(coll, 0) >= t}\n",
    "\n",
    "    sorted_collocations = sorted(filtered_collocations.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return sorted_collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'knesset_corpus.jsonl'\n",
    "output_file = 'top_collocations.txt'\n",
    "k = 10  # top 10 collocations\n",
    "n = 2   # bigrams\n",
    "t = 5   # minimum of <t> counts for an n-gram\n",
    "type = 'tfidf'\n",
    "\n",
    "#load corpus to df\n",
    "with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "corpus_df = pd.DataFrame(data)\n",
    "\n",
    "result = get_k_n_t_collocations(corpus_df, k, n, t, type)\n",
    "\n",
    "#save the output into a file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for collocation, score in result:\n",
    "        f.write(f\"{collocation}: {score}\\n\")\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Input: list of strings and a percentage x\n",
    "# Output: list of strings after masking x% of the tokens\n",
    "def mask_tokens_in_sentences(sentences, x):\n",
    "    masked_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        num_tokens_to_mask = int(len(tokens) * (x / 100))\n",
    "        tokens_to_mask = random.sample(range(len(tokens)), num_tokens_to_mask)\n",
    "\n",
    "        masked_tokens = [\"[*]\" if i in tokens_to_mask else token for i, token in enumerate(tokens)]\n",
    "        masked_sentences.append(\" \".join(masked_tokens))\n",
    "    return masked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a dataframe, amount of entries to mask with [*], and a percentage x\n",
    "# Output: the dataframe after applying the mask\n",
    "def mask_corpus(corpus_df, amount_to_mask, x):\n",
    "    if amount_to_mask > len(corpus_df):\n",
    "        amount_to_mask = len(corpus_df)\n",
    "\n",
    "    mask_indices = random.sample(range(len(corpus_df)), amount_to_mask)\n",
    "\n",
    "    corpus_df.loc[mask_indices, 'sentence_text'] = corpus_df.loc[mask_indices, 'sentence_text'].apply(\n",
    "        lambda sentence: mask_tokens_in_sentences(sentence, x)\n",
    "    )\n",
    "    return corpus_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
