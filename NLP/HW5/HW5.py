import os
import shutil
from transformers import (
    GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling
)
from datasets import load_from_disk, load_dataset
import torch


# Load IMDb Dataset
def load_imdb_dataset():
    if os.path.exists("save_dir/imdb_subset"):
        subset = load_from_disk("save_dir/imdb_subset")
    else:
        dataset = load_dataset("imdb")
        subset = dataset["train"].shuffle(seed=42).select(range(500))
        subset.save_to_disk("save_dir/imdb_subset")
    return subset


# Split Dataset into Positive and Negative Subsets with Sentiment Labels (100 samples each)
def split_and_label_dataset(dataset):
    positive_reviews = dataset.filter(lambda example: example["label"] == 1).select(range(100))
    negative_reviews = dataset.filter(lambda example: example["label"] == 0).select(range(100))

    # Add sentiment labels to text
    def add_label(example, label):
        example["text"] = f"{label}: {example['text']}"
        return example

    positive_reviews = positive_reviews.map(lambda x: add_label(x, "Positive"))
    negative_reviews = negative_reviews.map(lambda x: add_label(x, "Negative"))

    return positive_reviews, negative_reviews


# Tokenize Reviews
def tokenize_reviews(dataset, tokenizer, max_length=150):
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    def tokenize_function(examples):
        return tokenizer(
            examples['text'],
            padding="max_length",
            truncation=True,
            max_length=max_length
        )

    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
    return tokenized_dataset


# Fine-Tune GPT-2 Model
def train_gpt2_model(gpt2_model, gpt2_tokenizer, tokenized_reviews, sentiment):
    """
    Fine-tunes GPT-2 on the tokenized reviews for the specified sentiment.
    """
    training_args = TrainingArguments(
        output_dir=f"./gpt2_{sentiment}_results",
        eval_strategy="epoch",
        learning_rate=5e-5,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=8,
        num_train_epochs=3,
        weight_decay=0.01,
        logging_dir=f"./gpt2_{sentiment}_logs",
        save_total_limit=2,
        fp16=True,
        no_cuda=False,
    )

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=gpt2_tokenizer,
        mlm=False
    )

    train_test_split = tokenized_reviews.train_test_split(test_size=0.2, seed=42)
    train_dataset = train_test_split["train"]
    eval_dataset = train_test_split["test"]

    trainer = Trainer(
        model=gpt2_model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator
    )

    trainer.train()
    results = trainer.evaluate()
    print(f"GPT-2 ({sentiment}) Evaluation Loss:", results["eval_loss"])

    # Save the model and tokenizer
    save_dir = f"save_dir/gpt2_{sentiment}_model"
    trainer.model.save_pretrained(save_dir)
    gpt2_tokenizer.save_pretrained(save_dir)


# Generate Reviews and Write to File
def generate_and_save_reviews():
    # Load models
    pos_tokenizer = GPT2Tokenizer.from_pretrained("save_dir/gpt2_positive_model")
    pos_model = GPT2LMHeadModel.from_pretrained("save_dir/gpt2_positive_model")
    neg_tokenizer = GPT2Tokenizer.from_pretrained("save_dir/gpt2_negative_model")
    neg_model = GPT2LMHeadModel.from_pretrained("save_dir/gpt2_negative_model")

    prompt = "The movie was"
    reviews = {"positive": [], "negative": []}

    # Generate Positive Reviews
    for i in range(5):
        review = generate_reviews(pos_model, pos_tokenizer, f"Positive: {prompt}")
        reviews["positive"].append(review)

    # Generate Negative Reviews
    for i in range(5):
        review = generate_reviews(neg_model, neg_tokenizer, f"Negative: {prompt}")
        reviews["negative"].append(review)

    # Write to file
    with open("generated_reviews.txt", "w", encoding="utf-8") as f:
        f.write("Reviews generated by positive model:\n")
        for i, review in enumerate(reviews["positive"], 1):
            f.write(f"{i}. {review}\n")
        f.write("\nReviews generated by negative model:\n")
        for i, review in enumerate(reviews["negative"], 1):
            f.write(f"{i}. {review}\n")


# Generate Reviews Helper
def generate_reviews(gpt2_model, gpt2_tokenizer, prompt, max_length=150, temperature=0.7, top_k=50, top_p=0.9, repetition_penalty=1.2):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    gpt2_model = gpt2_model.to(device)

    input_ids = gpt2_tokenizer.encode(prompt, return_tensors="pt").to(device)
    attention_mask = input_ids.ne(gpt2_tokenizer.pad_token_id).to(device)

    with torch.no_grad():
        output = gpt2_model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_length,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            do_sample=True,
            num_return_sequences=1
        )

    return gpt2_tokenizer.decode(output[0], skip_special_tokens=True)


# Main Function
def prog_manager():
    # Remove old models if they exist
    shutil.rmtree("save_dir/gpt2_positive_model", ignore_errors=True)
    shutil.rmtree("save_dir/gpt2_negative_model", ignore_errors=True)

    # Load and process dataset
    dataset = load_imdb_dataset()
    positive_reviews, negative_reviews = split_and_label_dataset(dataset)

    # Load tokenizer and model
    gpt2_tokenizer, gpt2_model = GPT2Tokenizer.from_pretrained("gpt2"), GPT2LMHeadModel.from_pretrained("gpt2")

    # Tokenize and fine-tune models
    pos_tokenized = tokenize_reviews(positive_reviews, gpt2_tokenizer)
    train_gpt2_model(gpt2_model, gpt2_tokenizer, pos_tokenized, sentiment="positive")

    neg_tokenized = tokenize_reviews(negative_reviews, gpt2_tokenizer)
    train_gpt2_model(gpt2_model, gpt2_tokenizer, neg_tokenized, sentiment="negative")

    # Generate and save reviews
    generate_and_save_reviews()


if __name__ == "__main__":
    prog_manager()
